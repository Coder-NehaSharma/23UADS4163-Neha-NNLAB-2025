{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.9.7-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: absl-py in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow-datasets) (2.1.0)\n",
      "Collecting click (from tensorflow-datasets)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting dm-tree (from tensorflow-datasets)\n",
      "  Downloading dm_tree-0.1.9.tar.gz (35 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting immutabledict (from tensorflow-datasets)\n",
      "  Downloading immutabledict-4.2.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow-datasets) (2.0.2)\n",
      "Collecting promise (from tensorflow-datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.20 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow-datasets) (5.29.3)\n",
      "Requirement already satisfied: psutil in /Users/nehasharma/Library/Python/3.12/lib/python/site-packages (from tensorflow-datasets) (6.1.1)\n",
      "Collecting pyarrow (from tensorflow-datasets)\n",
      "  Downloading pyarrow-19.0.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow-datasets) (2.32.3)\n",
      "Collecting simple-parsing (from tensorflow-datasets)\n",
      "  Downloading simple_parsing-0.1.7-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets)\n",
      "  Downloading tensorflow_metadata-1.16.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: termcolor in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow-datasets) (2.5.0)\n",
      "Collecting toml (from tensorflow-datasets)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tqdm (from tensorflow-datasets)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow-datasets) (1.17.2)\n",
      "Collecting etils>=1.9.1 (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading etils-1.12.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fsspec (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting importlib_resources (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing_extensions in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (4.12.2)\n",
      "Collecting zipp (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow-datasets) (2025.1.31)\n",
      "Collecting attrs>=18.2.0 (from dm-tree->tensorflow-datasets)\n",
      "  Downloading attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: six in /Users/nehasharma/Library/Python/3.12/lib/python/site-packages (from promise->tensorflow-datasets) (1.17.0)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple-parsing->tensorflow-datasets)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow-datasets)\n",
      "  Downloading googleapis_common_protos-1.68.0-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Downloading tensorflow_datasets-4.9.7-py3-none-any.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading etils-1.12.0-py3-none-any.whl (166 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
      "Downloading pyarrow-19.0.1-cp312-cp312-macosx_12_0_arm64.whl (30.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading simple_parsing-0.1.7-py3-none-any.whl (112 kB)\n",
      "Downloading tensorflow_metadata-1.16.1-py3-none-any.whl (28 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading googleapis_common_protos-1.68.0-py2.py3-none-any.whl (164 kB)\n",
      "Downloading fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Building wheels for collected packages: dm-tree, promise\n",
      "  Building wheel for dm-tree (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dm-tree: filename=dm_tree-0.1.9-cp312-cp312-macosx_15_0_universal2.whl size=112039 sha256=552751569475c2ea05c11882c905db767ecfbc39358edba4affe0b965f2a1619\n",
      "  Stored in directory: /Users/nehasharma/Library/Caches/pip/wheels/89/01/af/d33b8877c63c2b90659a0eba4cffefbd049665c311d0a3d23a\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21548 sha256=a10b6dffa87d738aca938102ff540fad9a5d1c5614683797a0c3e5ae97837501\n",
      "  Stored in directory: /Users/nehasharma/Library/Caches/pip/wheels/e7/e6/28/864bdfee5339dbd6ddcb5a186286a8e217648ec198bdf0097d\n",
      "Successfully built dm-tree promise\n",
      "Installing collected packages: zipp, tqdm, toml, pyarrow, promise, importlib_resources, immutabledict, googleapis-common-protos, fsspec, etils, docstring-parser, click, attrs, tensorflow-metadata, simple-parsing, dm-tree, tensorflow-datasets\n",
      "Successfully installed attrs-25.1.0 click-8.1.8 dm-tree-0.1.9 docstring-parser-0.16 etils-1.12.0 fsspec-2025.2.0 googleapis-common-protos-1.68.0 immutabledict-4.2.1 importlib_resources-6.5.2 promise-2.3 pyarrow-19.0.1 simple-parsing-0.1.7 tensorflow-datasets-4.9.7 tensorflow-metadata-1.16.1 toml-0.10.2 tqdm-4.67.1 zipp-3.21.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective:\n",
    "To implement a three-layer neural network using TensorFlow (without Keras) for classifying handwritten digits from the MNIST dataset. The implementation should demonstrate feed-forward and back-propagation approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of the Model:\n",
    "The model consists of three layers:\n",
    "Input Layer: 784 neurons (28x28 flattened image pixels)\n",
    "Hidden Layer 1: 128 neurons, activation: sigmoid\n",
    "Hidden Layer 2: 64 neurons, activation: sigmoid\n",
    "Output Layer: 10 neurons (digits 0-9), producing logits\n",
    "Feed-forward propagation computes the output.\n",
    "Back-propagation updates weights using Stochastic Gradient Descent (SGD) with Momentum.\n",
    "The loss function used is Softmax Cross-Entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 16:36:46.692786: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /Users/nehasharma/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Dl Completed...: 100%|██████████| 5/5 [00:14<00:00,  2.81s/ file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to /Users/nehasharma/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset, info = tfds.load(\"mnist\", as_supervised=True, with_info=True)\n",
    "train_dataset, test_dataset = dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0 \n",
    "    image = tf.reshape(image, [-1])  \n",
    "    label = tf.one_hot(label, depth=10)  \n",
    "    return image, label\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess).shuffle(10000).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.map(preprocess).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define model parameters\n",
    "input_dim = 784\n",
    "hidden_dim1 = 128\n",
    "hidden_dim2 = 64\n",
    "output_dim = 10\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = tf.Variable(tf.random.normal([input_dim, hidden_dim1], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([hidden_dim1]))\n",
    "W2 = tf.Variable(tf.random.normal([hidden_dim1, hidden_dim2], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([hidden_dim2]))\n",
    "W3 = tf.Variable(tf.random.normal([hidden_dim2, output_dim], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([output_dim]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define forward pass\n",
    "def model(x):\n",
    "    hidden_layer1 = tf.sigmoid(tf.matmul(x, W1) + b1)  # First Hidden Layer\n",
    "    hidden_layer2 = tf.sigmoid(tf.matmul(hidden_layer1, W2) + b2)  # Second Hidden Layer\n",
    "    logits = tf.matmul(hidden_layer2, W3) + b3  # Output layer (logits)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function (Softmax Cross-Entropy)\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "# Accuracy function\n",
    "def compute_accuracy(dataset):\n",
    "    correct_preds, total_samples = 0, 0\n",
    "    for images, labels in dataset:\n",
    "        logits = model(images)\n",
    "        correct_preds += tf.reduce_sum(tf.cast(tf.equal(tf.argmax(logits, axis=1), tf.argmax(labels, axis=1)), tf.float32)).numpy()\n",
    "        total_samples += images.shape[0]\n",
    "    return correct_preds / total_samples\n",
    "\n",
    "# Optimizer (SGD with Momentum)\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training step function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(images)\n",
    "        loss = compute_loss(logits, labels)\n",
    "    gradients = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2, W3, b3]))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 16:39:08.403789: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:376] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2025-02-22 16:39:18.180031: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 671.4664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 16:39:27.719698: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 254.0650\n",
      "Epoch 3, Loss: 172.3769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 16:39:46.985768: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 128.9476\n",
      "Epoch 5, Loss: 103.9082\n",
      "Epoch 6, Loss: 81.9009\n",
      "Epoch 7, Loss: 64.9891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 16:40:24.741564: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 52.1213\n",
      "Epoch 9, Loss: 43.1821\n",
      "Epoch 10, Loss: 34.6094\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for images, labels in train_dataset:\n",
    "        loss = train_step(images, labels)\n",
    "        total_loss += loss.numpy()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Accuracy (SGD): 0.9963\n",
      "Final Test Accuracy (SGD): 0.9781\n"
     ]
    }
   ],
   "source": [
    "# Compute final training and test accuracy\n",
    "train_accuracy = compute_accuracy(train_dataset)\n",
    "test_accuracy = compute_accuracy(test_dataset)\n",
    "\n",
    "print(f\"Final Training Accuracy (SGD): {train_accuracy:.4f}\")\n",
    "print(f\"Final Test Accuracy (SGD): {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of the Code:\n",
    "\n",
    "Dataset Preprocessing:\n",
    "Loads MNIST dataset using tensorflow_datasets.\n",
    "Normalizes images (0 to 1 range).\n",
    "Flattens 28×28 images into 784 feature vectors.\n",
    "Converts labels into one-hot encoding (for softmax classification).\n",
    "Uses shuffling and batching for efficient training.\n",
    "\n",
    "Model Architecture:\n",
    "Weight matrices (W1, W2, W3) initialized randomly.\n",
    "Bias vectors (b1, b2, b3) initialized to zeros.\n",
    "Activation function: Sigmoid for hidden layers, Softmax for output.\n",
    "\n",
    "Feed-forward computation:\n",
    "First hidden layer: sigmoid(W1*x + b1)\n",
    "Second hidden layer: sigmoid(W2*hidden1 + b2)\n",
    "Output layer (logits): W3*hidden2 + b3\n",
    "\n",
    "Training Process:\n",
    "Loss function: softmax_cross_entropy_with_logits\n",
    "Optimizer: Stochastic Gradient Descent (SGD) with momentum (0.9)\n",
    "Gradient computation using tf.GradientTape().\n",
    "Backpropagation updates weights (W1, W2, W3) and biases.\n",
    "\n",
    "Performance Evaluation:\n",
    "Computes accuracy on training and test data.\n",
    "Outputs loss per epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "Successfully implemented a three-layer neural network using TensorFlow (without Keras).\n",
    "Demonstrated feed-forward propagation and backpropagation for training.\n",
    "Achieved reasonable accuracy on the MNIST dataset using SGD with Momentum.\n",
    "The model can be further improved using ReLU activations, batch normalization, or Adam optimizer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
