{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1236.5002\n",
      "Epoch 2, Loss: 526.6897\n",
      "Epoch 3, Loss: 353.7360\n",
      "Epoch 4, Loss: 253.7513\n",
      "Epoch 5, Loss: 185.6968\n",
      "Epoch 6, Loss: 132.8225\n",
      "Epoch 7, Loss: 87.4903\n",
      "Epoch 8, Loss: 62.7368\n",
      "Epoch 9, Loss: 33.7113\n",
      "Epoch 10, Loss: 26.2955\n",
      "Epoch 11, Loss: 15.6682\n",
      "Epoch 12, Loss: 10.4945\n",
      "Epoch 13, Loss: 8.2548\n",
      "Epoch 14, Loss: 6.6649\n",
      "Epoch 15, Loss: 5.8708\n",
      "Epoch 16, Loss: 5.2598\n",
      "Epoch 17, Loss: 4.7984\n",
      "Epoch 18, Loss: 4.3838\n",
      "Epoch 19, Loss: 3.9996\n",
      "Epoch 20, Loss: 3.7404\n",
      "Epoch 21, Loss: 3.5173\n",
      "Epoch 22, Loss: 3.2855\n",
      "Epoch 23, Loss: 3.1075\n",
      "Epoch 24, Loss: 2.9654\n",
      "Epoch 25, Loss: 2.8098\n",
      "Epoch 26, Loss: 2.6599\n",
      "Epoch 27, Loss: 2.5424\n",
      "Epoch 28, Loss: 2.4251\n",
      "Epoch 29, Loss: 2.3370\n",
      "Epoch 30, Loss: 2.2296\n",
      "Epoch 31, Loss: 2.1379\n",
      "Epoch 32, Loss: 2.0657\n",
      "Epoch 33, Loss: 1.9849\n",
      "Epoch 34, Loss: 1.9239\n",
      "Epoch 35, Loss: 1.8466\n",
      "Epoch 36, Loss: 1.7988\n",
      "Epoch 37, Loss: 1.7408\n",
      "Epoch 38, Loss: 1.6834\n",
      "Epoch 39, Loss: 1.6330\n",
      "Epoch 40, Loss: 1.5833\n",
      "Epoch 41, Loss: 1.5393\n",
      "Epoch 42, Loss: 1.4980\n",
      "Epoch 43, Loss: 1.4573\n",
      "Epoch 44, Loss: 1.4176\n",
      "Epoch 45, Loss: 1.3853\n",
      "Epoch 46, Loss: 1.3461\n",
      "Epoch 47, Loss: 1.3124\n",
      "Epoch 48, Loss: 1.2877\n",
      "Epoch 49, Loss: 1.2538\n",
      "Epoch 50, Loss: 1.2276\n",
      "Training Accuracy: 1.0000\n",
      "Test Accuracy: 0.9824\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "ds, info = tfds.load(\"mnist\", as_supervised=True, with_info=True)\n",
    "train_dataset, test_dataset = ds[\"train\"], ds[\"test\"]\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 50\n",
    "HIDDEN_UNITS = 256\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize\n",
    "    image = tf.reshape(image, [-1])  # Flatten to 784\n",
    "    label = tf.one_hot(label, depth=10)\n",
    "    return image, label\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess).shuffle(10000).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.map(preprocess).batch(BATCH_SIZE)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = tf.Variable(tf.random.normal([input_dim, HIDDEN_UNITS], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([HIDDEN_UNITS]))\n",
    "W2 = tf.Variable(tf.random.normal([HIDDEN_UNITS, output_dim], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([output_dim]))\n",
    "\n",
    "# Forward pass\n",
    "def model(x):\n",
    "    hidden = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "    logits = tf.matmul(hidden, W2) + b2\n",
    "    return logits\n",
    "\n",
    "# Loss function\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "# Accuracy function\n",
    "def compute_accuracy(dataset):\n",
    "    correct, total = 0, 0\n",
    "    for x, y in dataset:\n",
    "        logits = model(x)\n",
    "        correct += tf.reduce_sum(tf.cast(tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1)), tf.float32)).numpy()\n",
    "        total += x.shape[0]\n",
    "    return correct / total\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.optimizers.SGD(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Training step\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = compute_loss(logits, y)\n",
    "    grads = tape.gradient(loss, [W1, b1, W2, b2])\n",
    "    optimizer.apply_gradients(zip(grads, [W1, b1, W2, b2]))\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        total_loss += train_step(x_batch, y_batch).numpy()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "train_acc = compute_accuracy(train_dataset)\n",
    "test_acc = compute_accuracy(test_dataset)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6629.4944\n",
      "Epoch 2, Loss: 2703.2928\n",
      "Epoch 3, Loss: 2173.2896\n",
      "Epoch 4, Loss: 1928.7537\n",
      "Epoch 5, Loss: 1768.4554\n",
      "Epoch 6, Loss: 1647.1029\n",
      "Epoch 7, Loss: 1548.5870\n",
      "Epoch 8, Loss: 1463.8393\n",
      "Epoch 9, Loss: 1390.2822\n",
      "Epoch 10, Loss: 1325.1940\n",
      "Epoch 11, Loss: 1265.1961\n",
      "Epoch 12, Loss: 1213.1845\n",
      "Epoch 13, Loss: 1163.2986\n",
      "Epoch 14, Loss: 1118.5109\n",
      "Epoch 15, Loss: 1076.2018\n",
      "Epoch 16, Loss: 1037.7070\n",
      "Epoch 17, Loss: 1002.8625\n",
      "Epoch 18, Loss: 969.1000\n",
      "Epoch 19, Loss: 937.2247\n",
      "Epoch 20, Loss: 908.4850\n",
      "Epoch 21, Loss: 879.9602\n",
      "Epoch 22, Loss: 853.3430\n",
      "Epoch 23, Loss: 828.6375\n",
      "Epoch 24, Loss: 805.7184\n",
      "Epoch 25, Loss: 783.0736\n",
      "Epoch 26, Loss: 761.7456\n",
      "Epoch 27, Loss: 741.3450\n",
      "Epoch 28, Loss: 721.4231\n",
      "Epoch 29, Loss: 704.4133\n",
      "Epoch 30, Loss: 686.5747\n",
      "Epoch 31, Loss: 669.6128\n",
      "Epoch 32, Loss: 653.5264\n",
      "Epoch 33, Loss: 637.9632\n",
      "Epoch 34, Loss: 622.6029\n",
      "Epoch 35, Loss: 608.6524\n",
      "Epoch 36, Loss: 594.3446\n",
      "Epoch 37, Loss: 581.8305\n",
      "Epoch 38, Loss: 568.5124\n",
      "Epoch 39, Loss: 555.9770\n",
      "Epoch 40, Loss: 543.9228\n",
      "Epoch 41, Loss: 533.3027\n",
      "Epoch 42, Loss: 521.7230\n",
      "Epoch 43, Loss: 511.2008\n",
      "Epoch 44, Loss: 499.8547\n",
      "Epoch 45, Loss: 490.1009\n",
      "Epoch 46, Loss: 480.9676\n",
      "Epoch 47, Loss: 469.9306\n",
      "Epoch 48, Loss: 461.6390\n",
      "Epoch 49, Loss: 451.8474\n",
      "Epoch 50, Loss: 443.1749\n",
      "Training Accuracy: 0.9797\n",
      "Test Accuracy: 0.9701\n"
     ]
    }
   ],
   "source": [
    "#First Combination : Hidden Layers: (160, 100), Learning Rate: 0.001\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "ds, info = tfds.load(\"mnist\", as_supervised=True, with_info=True)\n",
    "train_dataset, test_dataset = ds[\"train\"], ds[\"test\"]\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 50\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.reshape(image, [-1])\n",
    "    label = tf.one_hot(label, depth=10)\n",
    "    return image, label\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess).shuffle(10000).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.map(preprocess).batch(BATCH_SIZE)\n",
    "\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal([input_dim, 160], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([160]))\n",
    "W2 = tf.Variable(tf.random.normal([160, 100], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([100]))\n",
    "W3 = tf.Variable(tf.random.normal([100, output_dim], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([output_dim]))\n",
    "\n",
    "def model(x):\n",
    "    hidden1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, W2) + b2)\n",
    "    logits = tf.matmul(hidden2, W3) + b3\n",
    "    return logits\n",
    "\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "def compute_accuracy(dataset):\n",
    "    correct, total = 0, 0\n",
    "    for x, y in dataset:\n",
    "        logits = model(x)\n",
    "        correct += tf.reduce_sum(tf.cast(tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1)), tf.float32)).numpy()\n",
    "        total += x.shape[0]\n",
    "    return correct / total\n",
    "\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = compute_loss(logits, y)\n",
    "    grads = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
    "    optimizer.apply_gradients(zip(grads, [W1, b1, W2, b2, W3, b3]))\n",
    "    return loss\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        total_loss += train_step(x_batch, y_batch).numpy()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "train_acc = compute_accuracy(train_dataset)\n",
    "test_acc = compute_accuracy(test_dataset)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2689.8076\n",
      "Epoch 2, Loss: 1293.6915\n",
      "Epoch 3, Loss: 960.7744\n",
      "Epoch 4, Loss: 771.6429\n",
      "Epoch 5, Loss: 651.0601\n",
      "Epoch 6, Loss: 556.9665\n",
      "Epoch 7, Loss: 491.0601\n",
      "Epoch 8, Loss: 428.8547\n",
      "Epoch 9, Loss: 385.7844\n",
      "Epoch 10, Loss: 346.3165\n",
      "Epoch 11, Loss: 310.9530\n",
      "Epoch 12, Loss: 276.7634\n",
      "Epoch 13, Loss: 253.1478\n",
      "Epoch 14, Loss: 225.5344\n",
      "Epoch 15, Loss: 205.9539\n",
      "Epoch 16, Loss: 187.3332\n",
      "Epoch 17, Loss: 167.7503\n",
      "Epoch 18, Loss: 149.6699\n",
      "Epoch 19, Loss: 137.9636\n",
      "Epoch 20, Loss: 124.3720\n",
      "Epoch 21, Loss: 110.6074\n",
      "Epoch 22, Loss: 100.1208\n",
      "Epoch 23, Loss: 88.6851\n",
      "Epoch 24, Loss: 80.7181\n",
      "Epoch 25, Loss: 72.9712\n",
      "Epoch 26, Loss: 64.9328\n",
      "Epoch 27, Loss: 57.0403\n",
      "Epoch 28, Loss: 51.9564\n",
      "Epoch 29, Loss: 46.0276\n",
      "Epoch 30, Loss: 40.5365\n",
      "Epoch 31, Loss: 37.7831\n",
      "Epoch 32, Loss: 33.8392\n",
      "Epoch 33, Loss: 30.7200\n",
      "Epoch 34, Loss: 27.5514\n",
      "Epoch 35, Loss: 24.8358\n",
      "Epoch 36, Loss: 22.6352\n",
      "Epoch 37, Loss: 21.7637\n",
      "Epoch 38, Loss: 19.4568\n",
      "Epoch 39, Loss: 18.1118\n",
      "Epoch 40, Loss: 16.3238\n",
      "Epoch 41, Loss: 14.9880\n",
      "Epoch 42, Loss: 14.0102\n",
      "Epoch 43, Loss: 13.1723\n",
      "Epoch 44, Loss: 12.3385\n",
      "Epoch 45, Loss: 11.5350\n",
      "Epoch 46, Loss: 10.9936\n",
      "Epoch 47, Loss: 10.4323\n",
      "Epoch 48, Loss: 10.0491\n",
      "Epoch 49, Loss: 9.4693\n",
      "Epoch 50, Loss: 8.9824\n",
      "Final Training Accuracy: 1.0000\n",
      "Final Test Accuracy: 0.9779\n"
     ]
    }
   ],
   "source": [
    "#Second Combination : Hidden Layers: (100, 100), Learning Rate: 0.001\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "dataset, info = tfds.load(\"mnist\", as_supervised=True, with_info=True)\n",
    "train_dataset, test_dataset = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.reshape(image, [-1])\n",
    "    label = tf.one_hot(label, depth=10)\n",
    "    return image, label\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess).shuffle(10000).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.map(preprocess).batch(BATCH_SIZE)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = 784\n",
    "hidden_dim1 = 100\n",
    "hidden_dim2 = 100\n",
    "output_dim = 10\n",
    "\n",
    "# Weights and biases\n",
    "W1 = tf.Variable(tf.random.normal([input_dim, hidden_dim1], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([hidden_dim1]))\n",
    "W2 = tf.Variable(tf.random.normal([hidden_dim1, hidden_dim2], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([hidden_dim2]))\n",
    "W3 = tf.Variable(tf.random.normal([hidden_dim2, output_dim], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([output_dim]))\n",
    "\n",
    "# Forward pass\n",
    "def model(x):\n",
    "    hidden_layer1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "    hidden_layer2 = tf.nn.relu(tf.matmul(hidden_layer1, W2) + b2)\n",
    "    logits = tf.matmul(hidden_layer2, W3) + b3\n",
    "    return logits\n",
    "\n",
    "# Loss\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "# Accuracy\n",
    "def compute_accuracy(dataset):\n",
    "    correct_preds, total_samples = 0, 0\n",
    "    for images, labels in dataset:\n",
    "        logits = model(images)\n",
    "        correct_preds += tf.reduce_sum(\n",
    "            tf.cast(tf.equal(tf.argmax(logits, axis=1), tf.argmax(labels, axis=1)), tf.float32)\n",
    "        ).numpy()\n",
    "        total_samples += images.shape[0]\n",
    "    return correct_preds / total_samples\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
    "\n",
    "# Training step\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(images)\n",
    "        loss = compute_loss(logits, labels)\n",
    "    gradients = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2, W3, b3]))\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for images, labels in train_dataset:\n",
    "        loss = train_step(images, labels)\n",
    "        total_loss += loss.numpy()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Final Accuracy\n",
    "train_accuracy = compute_accuracy(train_dataset)\n",
    "test_accuracy = compute_accuracy(test_dataset)\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 12708.3774\n",
      "Epoch 2, Loss: 13936.1695\n",
      "Epoch 3, Loss: 13943.1357\n",
      "Epoch 4, Loss: 13946.1079\n",
      "Epoch 5, Loss: 13943.6817\n",
      "Epoch 6, Loss: 13942.0946\n",
      "Epoch 7, Loss: 13944.3285\n",
      "Epoch 8, Loss: 13939.1488\n",
      "Epoch 9, Loss: 13942.9906\n",
      "Epoch 10, Loss: 13944.3337\n",
      "Epoch 11, Loss: 13947.6421\n",
      "Epoch 12, Loss: 13939.2160\n",
      "Epoch 13, Loss: 13948.8333\n",
      "Epoch 14, Loss: 13942.3898\n",
      "Epoch 15, Loss: 13947.9921\n",
      "Epoch 16, Loss: 13952.3817\n",
      "Epoch 17, Loss: 13942.8294\n",
      "Epoch 18, Loss: 13945.2337\n",
      "Epoch 19, Loss: 13945.3121\n",
      "Epoch 20, Loss: 13943.2357\n",
      "Epoch 21, Loss: 13947.4088\n",
      "Epoch 22, Loss: 13939.5406\n",
      "Epoch 23, Loss: 13939.1945\n",
      "Epoch 24, Loss: 13941.3369\n",
      "Epoch 25, Loss: 13937.5437\n",
      "Epoch 26, Loss: 13940.6041\n",
      "Epoch 27, Loss: 13948.4225\n",
      "Epoch 28, Loss: 13946.8216\n",
      "Epoch 29, Loss: 13940.1649\n",
      "Epoch 30, Loss: 13943.2302\n",
      "Epoch 31, Loss: 13939.6182\n",
      "Epoch 32, Loss: 13943.6881\n",
      "Epoch 33, Loss: 13943.9917\n",
      "Epoch 34, Loss: 13945.4014\n",
      "Epoch 35, Loss: 13940.7708\n",
      "Epoch 36, Loss: 13938.9744\n",
      "Epoch 37, Loss: 13944.9024\n",
      "Epoch 38, Loss: 13943.3616\n",
      "Epoch 39, Loss: 13944.3516\n",
      "Epoch 40, Loss: 13947.7702\n",
      "Epoch 41, Loss: 13941.1300\n",
      "Epoch 42, Loss: 13942.3473\n",
      "Epoch 43, Loss: 13949.1252\n",
      "Epoch 44, Loss: 13940.5693\n",
      "Epoch 45, Loss: 13945.1799\n",
      "Epoch 46, Loss: 13939.8725\n",
      "Epoch 47, Loss: 13945.3498\n",
      "Epoch 48, Loss: 13946.8243\n",
      "Epoch 49, Loss: 13940.3291\n",
      "Epoch 50, Loss: 13942.9152\n",
      "Final Training Accuracy: 0.1124\n",
      "Final Test Accuracy: 0.1135\n"
     ]
    }
   ],
   "source": [
    "#Third Combination : Hidden Layers: (100, 100), Learning Rate: 0.1\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "dataset, info = tfds.load(\"mnist\", as_supervised=True, with_info=True)\n",
    "train_dataset, test_dataset = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.reshape(image, [-1])\n",
    "    label = tf.one_hot(label, depth=10)\n",
    "    return image, label\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess).shuffle(10000).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.map(preprocess).batch(BATCH_SIZE)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = 784\n",
    "hidden_dim1 = 100\n",
    "hidden_dim2 = 100\n",
    "output_dim = 10\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = tf.Variable(tf.random.normal([input_dim, hidden_dim1], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([hidden_dim1]))\n",
    "W2 = tf.Variable(tf.random.normal([hidden_dim1, hidden_dim2], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([hidden_dim2]))\n",
    "W3 = tf.Variable(tf.random.normal([hidden_dim2, output_dim], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([output_dim]))\n",
    "\n",
    "# Forward pass\n",
    "def model(x):\n",
    "    hidden_layer1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "    hidden_layer2 = tf.nn.relu(tf.matmul(hidden_layer1, W2) + b2)\n",
    "    logits = tf.matmul(hidden_layer2, W3) + b3\n",
    "    return logits\n",
    "\n",
    "# Loss function\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "# Accuracy function\n",
    "def compute_accuracy(dataset):\n",
    "    correct_preds, total_samples = 0, 0\n",
    "    for images, labels in dataset:\n",
    "        logits = model(images)\n",
    "        correct_preds += tf.reduce_sum(\n",
    "            tf.cast(tf.equal(tf.argmax(logits, axis=1), tf.argmax(labels, axis=1)), tf.float32)\n",
    "        ).numpy()\n",
    "        total_samples += images.shape[0]\n",
    "    return correct_preds / total_samples\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "\n",
    "# Training step\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(images)\n",
    "        loss = compute_loss(logits, labels)\n",
    "    gradients = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2, W3, b3]))\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for images, labels in train_dataset:\n",
    "        loss = train_step(images, labels)\n",
    "        total_loss += loss.numpy()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "train_accuracy = compute_accuracy(train_dataset)\n",
    "test_accuracy = compute_accuracy(test_dataset)\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 15323.1669\n",
      "Epoch 2, Loss: 15234.9803\n",
      "Epoch 3, Loss: 15468.4656\n",
      "Epoch 4, Loss: 15249.9448\n",
      "Epoch 5, Loss: 15296.8790\n",
      "Epoch 6, Loss: 15310.8297\n",
      "Epoch 7, Loss: 15340.0094\n",
      "Epoch 8, Loss: 15282.5170\n",
      "Epoch 9, Loss: 15314.9356\n",
      "Epoch 10, Loss: 15287.7777\n",
      "Epoch 11, Loss: 15294.5677\n",
      "Epoch 12, Loss: 15284.1191\n",
      "Epoch 13, Loss: 15289.7431\n",
      "Epoch 14, Loss: 15322.3182\n",
      "Epoch 15, Loss: 15294.1694\n",
      "Epoch 16, Loss: 15268.8878\n",
      "Epoch 17, Loss: 15317.5412\n",
      "Epoch 18, Loss: 15266.9840\n",
      "Epoch 19, Loss: 15318.1845\n",
      "Epoch 20, Loss: 15298.5966\n",
      "Epoch 21, Loss: 15294.5437\n",
      "Epoch 22, Loss: 15307.0999\n",
      "Epoch 23, Loss: 15267.2593\n",
      "Epoch 24, Loss: 15300.7331\n",
      "Epoch 25, Loss: 15249.0122\n",
      "Epoch 26, Loss: 15260.6246\n",
      "Epoch 27, Loss: 15329.3653\n",
      "Epoch 28, Loss: 15272.5815\n",
      "Epoch 29, Loss: 15304.8840\n",
      "Epoch 30, Loss: 15296.4084\n",
      "Epoch 31, Loss: 15303.9543\n",
      "Epoch 32, Loss: 15275.1298\n",
      "Epoch 33, Loss: 15326.1797\n",
      "Epoch 34, Loss: 15278.3619\n",
      "Epoch 35, Loss: 15317.1415\n",
      "Epoch 36, Loss: 15290.5596\n",
      "Epoch 37, Loss: 15328.3029\n",
      "Epoch 38, Loss: 15253.1955\n",
      "Epoch 39, Loss: 15275.9081\n",
      "Epoch 40, Loss: 15317.0367\n",
      "Epoch 41, Loss: 15240.5292\n",
      "Epoch 42, Loss: 15317.9535\n",
      "Epoch 43, Loss: 15267.8982\n",
      "Epoch 44, Loss: 15256.8850\n",
      "Epoch 45, Loss: 15242.9103\n",
      "Epoch 46, Loss: 15274.6194\n",
      "Epoch 47, Loss: 15395.1757\n",
      "Epoch 48, Loss: 15261.5100\n",
      "Epoch 49, Loss: 15317.2062\n",
      "Epoch 50, Loss: 15238.7626\n",
      "Final Training Accuracy: 0.0993\n",
      "Final Test Accuracy: 0.1032\n"
     ]
    }
   ],
   "source": [
    "#Fouth Combination : Hidden Layers: (100, 100), Learning Rate: 1\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "dataset, info = tfds.load(\"mnist\", as_supervised=True, with_info=True)\n",
    "train_dataset, test_dataset = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.reshape(image, [-1])\n",
    "    label = tf.one_hot(label, depth=10)\n",
    "    return image, label\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess).shuffle(10000).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.map(preprocess).batch(BATCH_SIZE)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = 784\n",
    "hidden_dim1 = 100\n",
    "hidden_dim2 = 100\n",
    "output_dim = 10\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = tf.Variable(tf.random.normal([input_dim, hidden_dim1], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([hidden_dim1]))\n",
    "W2 = tf.Variable(tf.random.normal([hidden_dim1, hidden_dim2], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([hidden_dim2]))\n",
    "W3 = tf.Variable(tf.random.normal([hidden_dim2, output_dim], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([output_dim]))\n",
    "\n",
    "# Forward pass\n",
    "def model(x):\n",
    "    hidden_layer1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "    hidden_layer2 = tf.nn.relu(tf.matmul(hidden_layer1, W2) + b2)\n",
    "    logits = tf.matmul(hidden_layer2, W3) + b3\n",
    "    return logits\n",
    "\n",
    "# Loss function\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "# Accuracy function\n",
    "def compute_accuracy(dataset):\n",
    "    correct_preds, total_samples = 0, 0\n",
    "    for images, labels in dataset:\n",
    "        logits = model(images)\n",
    "        correct_preds += tf.reduce_sum(\n",
    "            tf.cast(tf.equal(tf.argmax(logits, axis=1), tf.argmax(labels, axis=1)), tf.float32)\n",
    "        ).numpy()\n",
    "        total_samples += images.shape[0]\n",
    "    return correct_preds / total_samples\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.optimizers.SGD(learning_rate=1.0, momentum=0.9)\n",
    "\n",
    "# Training step\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(images)\n",
    "        loss = compute_loss(logits, labels)\n",
    "    gradients = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2, W3, b3]))\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for images, labels in train_dataset:\n",
    "        loss = train_step(images, labels)\n",
    "        total_loss += loss.numpy()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "train_accuracy = compute_accuracy(train_dataset)\n",
    "test_accuracy = compute_accuracy(test_dataset)\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
